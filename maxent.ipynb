{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_sentence(sent):\n",
    "    return({word:True for word in nltk.word_tokenize(sent)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': True, 'cat': True, 'is': True, 'very': True, 'cute': True}\n"
     ]
    }
   ],
   "source": [
    "print(format_sentence(\"The cat is very cute\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = []\n",
    "with open(\"./pos_tweets.txt\") as f:\n",
    "    for i in f:\n",
    "        pos.append([format_sentence(i),'pos'])\n",
    "        \n",
    "neg = []\n",
    "with open(\"./neg_tweets.txt\") as f:\n",
    "    for i in f:\n",
    "        neg.append([format_sentence(i),'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = pos[:int((.8)*len(pos))]+neg[:int((.8)*len(neg))]\n",
    "test = pos[int((.8)*len(pos)):]+neg[int((.8)*len(neg)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.308\n",
      "             2          -0.47102        0.741\n",
      "             3          -0.39234        0.870\n",
      "             4          -0.33735        0.931\n",
      "             5          -0.29685        0.956\n",
      "             6          -0.26576        0.972\n",
      "             7          -0.24106        0.981\n",
      "             8          -0.22092        0.987\n",
      "             9          -0.20412        0.988\n",
      "            10          -0.18988        0.989\n",
      "            11          -0.17762        0.992\n",
      "            12          -0.16696        0.993\n",
      "            13          -0.15758        0.993\n",
      "            14          -0.14925        0.994\n",
      "            15          -0.14181        0.994\n",
      "            16          -0.13512        0.994\n",
      "            17          -0.12905        0.994\n",
      "            18          -0.12354        0.994\n",
      "            19          -0.11850        0.995\n",
      "            20          -0.11387        0.995\n",
      "            21          -0.10961        0.995\n",
      "            22          -0.10566        0.995\n",
      "            23          -0.10201        0.995\n",
      "            24          -0.09860        0.995\n",
      "            25          -0.09543        0.995\n",
      "            26          -0.09246        0.995\n",
      "            27          -0.08967        0.995\n",
      "            28          -0.08706        0.996\n",
      "            29          -0.08459        0.997\n",
      "            30          -0.08227        0.998\n",
      "            31          -0.08008        0.998\n",
      "            32          -0.07800        0.998\n",
      "            33          -0.07604        0.998\n",
      "            34          -0.07417        0.998\n",
      "            35          -0.07239        0.998\n",
      "            36          -0.07070        0.998\n",
      "            37          -0.06909        0.998\n",
      "            38          -0.06755        0.998\n",
      "            39          -0.06609        0.998\n",
      "            40          -0.06468        0.998\n",
      "            41          -0.06334        0.998\n",
      "            42          -0.06205        0.998\n",
      "            43          -0.06082        0.999\n",
      "            44          -0.05963        0.999\n",
      "            45          -0.05849        0.999\n",
      "            46          -0.05740        0.999\n",
      "            47          -0.05634        0.999\n",
      "            48          -0.05533        0.999\n",
      "            49          -0.05435        0.999\n",
      "            50          -0.05341        0.999\n",
      "            51          -0.05250        0.999\n",
      "            52          -0.05162        0.999\n",
      "            53          -0.05077        0.999\n",
      "            54          -0.04995        0.999\n",
      "            55          -0.04916        0.999\n",
      "            56          -0.04839        0.999\n",
      "            57          -0.04764        0.999\n",
      "            58          -0.04692        0.999\n",
      "            59          -0.04622        0.999\n",
      "            60          -0.04554        0.999\n",
      "            61          -0.04489        0.999\n",
      "            62          -0.04425        0.999\n",
      "            63          -0.04363        0.999\n",
      "            64          -0.04302        0.999\n",
      "            65          -0.04244        0.999\n",
      "            66          -0.04187        0.999\n",
      "            67          -0.04131        0.999\n",
      "            68          -0.04077        0.999\n",
      "            69          -0.04025        0.999\n",
      "            70          -0.03974        0.999\n",
      "            71          -0.03924        0.999\n",
      "            72          -0.03875        0.999\n",
      "            73          -0.03828        0.999\n",
      "            74          -0.03782        0.999\n",
      "            75          -0.03737        0.999\n",
      "            76          -0.03693        0.999\n",
      "            77          -0.03650        1.000\n",
      "            78          -0.03608        1.000\n",
      "            79          -0.03567        1.000\n",
      "            80          -0.03527        1.000\n",
      "            81          -0.03487        1.000\n",
      "            82          -0.03449        1.000\n",
      "            83          -0.03412        1.000\n",
      "            84          -0.03375        1.000\n",
      "            85          -0.03339        1.000\n",
      "            86          -0.03304        1.000\n",
      "            87          -0.03270        1.000\n",
      "            88          -0.03237        1.000\n",
      "            89          -0.03204        1.000\n",
      "            90          -0.03172        1.000\n",
      "            91          -0.03140        1.000\n",
      "            92          -0.03109        1.000\n",
      "            93          -0.03079        1.000\n",
      "            94          -0.03049        1.000\n",
      "            95          -0.03020        1.000\n",
      "            96          -0.02991        1.000\n",
      "            97          -0.02963        1.000\n",
      "            98          -0.02936        1.000\n",
      "            99          -0.02909        1.000\n",
      "         Final          -0.02883        1.000\n",
      "   5.810 humming==True and label is 'pos'\n",
      "   5.710 dashboard==True and label is 'pos'\n",
      "   5.447 GuiPulp==True and label is 'pos'\n",
      "  -5.327 no==True and label is 'pos'\n",
      "   5.176 ThePartyScene==True and label is 'pos'\n",
      "   5.152 FOLLOW==True and label is 'pos'\n",
      "   4.985 Ash==True and label is 'neg'\n",
      "   4.973 Finally==True and label is 'pos'\n",
      "   4.901 acacia_scott==True and label is 'neg'\n",
      "   4.901 dougiemcfly==True and label is 'neg'\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import MaxentClassifier as mc\n",
    "classifier = mc.train(training)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "example = \"Cats are awesome!\"\n",
    "print(classifier.classify(format_sentence(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "example = \"I don't love cats.\"\n",
    "print(classifier.classify(format_sentence(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "example = \"Don't thank me!\"\n",
    "print(classifier.classify(format_sentence(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7985074626865671\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
